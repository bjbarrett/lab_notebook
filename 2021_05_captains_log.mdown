
# 03.05.2021
ToDo:
1. ODS Stuff
	1. ~~ReadMe Example~~
	2. ~~Project Metadata Template~~
	3. Exiftool Tutorial
	4. Hashing Tutorial
2. ~~Train Evelyn (cc Sylvia)~~
3. Inventory for Chuck etal
4. Get Angie Camera Numbers
5. Press release
6. Tamara and Meredith Agouti
7. ~~Troubleshooting with hyperfire mpo4 video~~
9. perr review an beh
10. sloanea paper
11. julia model
12. aplin simulations
13. rewrite DAGs ans Kristen paper results
14. email cam trap project

I realized the effed up videos in the Reconyx Coiba stuff, r7 and r8 are due to the hyperfire version 2 being used for video

# 04.05.2021

need to recap
Worked on Kristen paper, got Kate new reaster removed the weird village name.


# 05.05.2021
 Dentist in AM.


 1. ODS Stuff
	1. Exiftool Tutorial
	2. Hashing Tutorial
13. rewrite DAGs ans Kristen paper results
3. Inventory for Chuck etal
4. Get Angie Camera Numbers
5. Press release
6. Tamara and Meredith Agouti
7. email susan mou
9. perr review an beh
10. sloanea paper
11. julia model
12. aplin simulations
14. email cam trap project

## Evelyn Meeting
1. Things to share:
 1. ppt
 2. excel
 3. doc
 4. agouti tour
 5. R repository
 6. slack tour
2. Assign a first non-tool deployment
3. Give a tool deployment, that I will inspect. SURVEY-CEBUS-03-04
4. Have her evaluate on of the tool site deployments as well.

We are scheduling a meeting for next week to check back in.

## Camera Numbers for Coiba
16 Ultrafires (need 4 each site, 2 experiments)
32 Hyperfires ()



# 06.05.2021

 1. ODS Stuff
	1. Exiftool Tutorial
	2. ~Finish Hashing Tutorial~
2. rewrite DAGs ans Kristen paper results
3. Inventory for Chuck etal
4. ~~Get Angie Camera Numbers~~
5. Press release
6. Tamara and Meredith Agouti
7. ~email susan mou~
9. perr review an beh
10. sloanea paper
11. julia model
12. aplin simulations
13. Email Damien Questions
14. ~~email cam trap project~~
15. ~kate wkspc~

git remote add origin https://github.com/bjbarrett/hashing_digest_tutorial.git
git branch -M main
git push -u origin main


library(stringr)
library(digest)

# list.files(path="/Users/sifaka/Dropbox/Wildbook_no_susan/BJBPhotos/" , full.names=TRUE  , recursive=TRUE , pattern=".JPG" )
# list.files(path="/Users/sifaka/Dropbox/Wildbook_no_susan/BJBPhotos/QJ"   , recursive=TRUE  )
# list.files(path="/Users/sifaka/Dropbox/WildbookCollaboration_BJB-SEP/GS"   , recursive=TRUE  )
##explore susan folders
susan_files <- list.files(path="/Users/sifaka/Dropbox/WildbookCollaboration_BJB-SEP/"   , recursive=TRUE  )
df1 <- data.frame (file  = susan_files,
                   person= rep("SEP" , length("susan_files"))
)
df1$individual <- substr(df1$file, start = 1, stop = 2)
df1$year <- substr(df1$file, start = 4, stop = 7)
#give each file a hash
df1$sha1_hash <- rep(NA, nrow(df1))
list_2 <- list.files(path="/Users/sifaka/Dropbox/WildbookCollaboration_BJB-SEP/" , full.names=TRUE  , recursive=TRUE  )

for(i in 1:length(list_2)){
  df1$sha1_hash[i] <-digest(list_2[i], algo="sha1", file=TRUE)
}

###explore brendan folders
brendan_files <- 
